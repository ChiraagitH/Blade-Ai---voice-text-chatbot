{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55221b1a",
   "metadata": {},
   "source": [
    "### 1)First Code Written:\n",
    "      Chat with LLaMa3 (Local Server)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52c6204f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA 3: You entered the number 3! Would you like to do something with it, like add or multiply?\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json={\n",
    "                \"model\": \"llama3\",\n",
    "                \"prompt\": user_input,\n",
    "                \"stream\": False\n",
    "            },\n",
    "            timeout=30\n",
    "        )\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            print(\"LLaMA 3:\", data.get(\"response\", \"\").strip())\n",
    "        else:\n",
    "            print(\"Error:\", response.status_code, response.text)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Request failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710a6ba8",
   "metadata": {},
   "source": [
    "### 2) Started Segreting Codes for Accuracy\n",
    "     Step 1: Sends the userâ€™s input to LLaMA3 running locally via Ollama\n",
    "      \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94361d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def chat_with_llama3(prompt):\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json={\n",
    "                \"model\": \"llama3\",\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False\n",
    "            },\n",
    "            timeout=180\n",
    "        )\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return response.json()['response']\n",
    "        else:\n",
    "            return f\"Error: {response.status_code} - {response.text}\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Request failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d229e4d4",
   "metadata": {},
   "source": [
    "### Step 2:\n",
    "    Gradio: Builds a user-friendly UI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286b6981",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dac1fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn = chat_with_llama3,\n",
    "    inputs='text',\n",
    "    outputs='text',\n",
    "    title='Blade Ai',\n",
    "    description='Your local chatbot powered by the LLaMa 3 running on Ollama' \n",
    ")\n",
    "\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c983efeb",
   "metadata": {},
   "source": [
    "### Step 3:\n",
    "    speech_recognition: Captures and converts speech to text\n",
    "    pyttsx3: Converts text responses to voice (offline TTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b35b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install SpeechRecognition pyttsx3 pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d4cf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "\n",
    "engine = pyttsx3.init()\n",
    "r = sr.Recognizer()\n",
    "\n",
    "def speak(text):\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()   \n",
    "\n",
    "def listen():\n",
    "    with sr.Microphone() as source:\n",
    "        print('Speak now...')\n",
    "        audio = r.listen(source)\n",
    "        try:\n",
    "            text = r.recognize_google(audio)\n",
    "            print('You said', query)\n",
    "            return query\n",
    "        except:\n",
    "            print(\"Sorry I couldn't hear you\")\n",
    "            return \"\"\n",
    "                 \n",
    "while True:\n",
    "    query = listen()\n",
    "    if query.lower() in ['exit','quit']:\n",
    "        break\n",
    "    response = chat_with_llama3(query)\n",
    "    speak(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37ee328",
   "metadata": {},
   "source": [
    "### step 4: Blade Ai\n",
    "    LLaMA3-powered voice & text chatbot with Gradio and local Ollama integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d4d933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chiraag\\Desktop\\m\\venv\\Lib\\site-packages\\jedi\\third_party\\typeshed\\stdlib\\3\\venv\\venv1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\chiraag\\AppData\\Local\\Temp\\ipykernel_14744\\3703493296.py:47: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¤ Listening...\n",
      "Sorry, I didn't catch that.\n",
      "ðŸŽ¤ Listening...\n",
      "You said: a how are you\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "import requests\n",
    "import gradio as gr\n",
    "\n",
    "engine = pyttsx3.init()\n",
    "r = sr.Recognizer()\n",
    "\n",
    "def chat_with_llama3(prompt):\n",
    "    response = requests.post(\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        json={\"model\": \"llama3\", \"prompt\": prompt, \"stream\": False}\n",
    "    )\n",
    "    return response.json()[\"response\"]\n",
    "\n",
    "def speak(text):\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "def listen():\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"ðŸŽ¤ Listening...\")\n",
    "        audio = r.listen(source)\n",
    "        try:\n",
    "            query = r.recognize_google(audio)\n",
    "            print(\"You said:\", query)\n",
    "            return query\n",
    "        except:\n",
    "            print(\"Sorry, I didn't catch that.\")\n",
    "            return \"\"\n",
    "\n",
    "def voice_interaction(history):\n",
    "    query = listen()\n",
    "    if query:\n",
    "        response = chat_with_llama3(query)\n",
    "        speak(response)\n",
    "        history= history + [(query, response)]    \n",
    "    return history\n",
    "\n",
    "def text_chat(user_input, history):\n",
    "    response = chat_with_llama3(user_input)\n",
    "    return response\n",
    "\n",
    "with gr.Blocks() as app:\n",
    "    gr.Markdown(\"Blade Ai\")\n",
    "    \n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox(label='Type your message...')\n",
    "    submit_btn = gr.Button('send')\n",
    "    voice_btn = gr.Button('ðŸŽ¤Voice input')\n",
    "\n",
    "    def respond(user_input, History):\n",
    "        response = text_chat(user_input, History)\n",
    "        History.append((user_input,response))\n",
    "        return '', History\n",
    "\n",
    "    submit_btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "\n",
    "\n",
    "    voice_btn.click(voice_interaction, inputs=[chatbot], outputs=[chatbot])\n",
    "\n",
    "app.launch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e92c4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790de103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7710966f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b360c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab6add2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa470d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9566a9dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37a6215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8e35df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfde522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aa11ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913fe308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc842c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ae92613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chiraag\\Desktop\\m\\venv\\Lib\\site-packages\\jedi\\third_party\\typeshed\\stdlib\\3\\venv\\venv1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import gradio as gr\n",
    "\n",
    "# Define the function directly\n",
    "def chat_with_llama3(prompt):\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json={\n",
    "                \"model\": \"llama3\",\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False\n",
    "            },\n",
    "            timeout=30\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"response\"]\n",
    "        else:\n",
    "            return f\"Error: {response.status_code} - {response.text}\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Request failed: {e}\"\n",
    "\n",
    "# Launch Gradio UI\n",
    "iface = gr.Interface(\n",
    "    fn=chat_with_llama3,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"Namma Bot\",\n",
    "    description=\"Your local chatbot powered by LLaMA 3 running on Ollama\"\n",
    ")\n",
    "\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec72f75c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
